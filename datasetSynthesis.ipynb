{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Altitude  Distance_from_Human_Paths  Livestock_Density  \\\n",
      "0  2023.988261                 434.678578           0.634095   \n",
      "1  3291.571474                 374.299977           0.132676   \n",
      "2  2810.386672                 216.189749           0.477984   \n",
      "3  2517.048665                 339.831903           0.386706   \n",
      "4  1543.241009                 227.141237           0.654869   \n",
      "\n",
      "   Vegetation_Diversity_Index  Water_Source_Availability  \\\n",
      "0                    0.319653                          0   \n",
      "1                    0.604097                          1   \n",
      "2                    0.157106                          0   \n",
      "3                    0.560503                          1   \n",
      "4                    0.349722                          1   \n",
      "\n",
      "   Human_Disturbance_Index      Slope  Annual_Rainfall  Bamboo_Coverage  \\\n",
      "0                 1.025552   1.902428      1364.685583        30.270545   \n",
      "1                 1.032245  16.484248      1616.727901        71.382519   \n",
      "2                 0.963946  29.466434      1373.229653        31.969303   \n",
      "3                 1.142016   9.966878      1747.708413        50.076009   \n",
      "4                 1.050573  10.647234      1500.073131        17.449099   \n",
      "\n",
      "   Suitable  Mean_Annual_Temperature  \n",
      "0         0                 9.912647  \n",
      "1         0                 1.701986  \n",
      "2         0                 6.258988  \n",
      "3         0                 7.423174  \n",
      "4         0                12.331676  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples in the dataset\n",
    "n_samples = 100000\n",
    "\n",
    "# Generate the synthetic data\n",
    "data = pd.DataFrame({\n",
    "    'Altitude': np.random.uniform(1200, 3400, n_samples),  # Uniform distribution\n",
    "    'Distance_from_Human_Paths': np.random.exponential(500, n_samples),  # Exponential distribution\n",
    "    'Livestock_Density': np.random.gamma(2, 0.5, n_samples),  # Gamma distribution for skewed data\n",
    "    'Vegetation_Diversity_Index': np.random.uniform(0, 1, n_samples),  # Uniform distribution\n",
    "    'Water_Source_Availability': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),  # Binary, with more 1s\n",
    "    'Human_Disturbance_Index': np.random.uniform(0, 1, n_samples),  # Uniform distribution\n",
    "    'Slope': np.random.uniform(0, 30, n_samples),  # Uniform distribution\n",
    "    'Annual_Rainfall': np.random.normal(1500, 250, n_samples)  # Normal distribution\n",
    "})\n",
    "\n",
    "# Add noise function\n",
    "def add_noise(factor, size):\n",
    "    return np.random.normal(1, factor, size)\n",
    "\n",
    "# Correlation 1: Altitude and Bamboo Coverage (with noise)\n",
    "data['Bamboo_Coverage'] = np.interp(data['Altitude'], [1200, 3400], [0, 100])\n",
    "data['Bamboo_Coverage'] *= add_noise(0.1, n_samples)  # Adding 10% noise\n",
    "\n",
    "# Correlation 2: Livestock Density and Vegetation Diversity Index (with noise)\n",
    "data['Vegetation_Diversity_Index'] *= np.exp(-data['Livestock_Density'])\n",
    "data['Vegetation_Diversity_Index'] *= add_noise(0.05, n_samples)  # Adding 5% noise\n",
    "\n",
    "# Correlation 3: Water Source Availability and Annual Rainfall (with noise)\n",
    "data['Water_Source_Availability'] = np.where(data['Annual_Rainfall'] > data['Annual_Rainfall'].mean(), 1, 0)\n",
    "data['Water_Source_Availability'] = np.where(np.random.rand(n_samples) < 0.1, 1 - data['Water_Source_Availability'], data['Water_Source_Availability'])  # Inverting 10% to add noise\n",
    "\n",
    "# Correlation 4: Distance from Human Paths and Human Disturbance Index (with noise)\n",
    "data['Human_Disturbance_Index'] = np.interp(data['Distance_from_Human_Paths'], [0, max(data['Distance_from_Human_Paths'])], [1, 0])\n",
    "data['Human_Disturbance_Index'] *= add_noise(0.1, n_samples)  # Adding 10% noise\n",
    "\n",
    "# Correlation 5: Slope and Bamboo Coverage (with noise)\n",
    "data['Bamboo_Coverage'] *= np.where(data['Slope'] < 20, 1, 0.5)  # Reduce bamboo coverage on steep slopes\n",
    "data['Bamboo_Coverage'] *= add_noise(0.1, n_samples)  # Adding 10% noise\n",
    "\n",
    "# Define the suitability based on simplified conditions (from the previous example)\n",
    "# Adjust thresholds if needed to accommodate correlations\n",
    "data['Suitable'] = np.where(\n",
    "    (data['Altitude'] > 1500) & \n",
    "    (data['Altitude'] < 3000) &\n",
    "    (data['Distance_from_Human_Paths'] > 500) &\n",
    "    (data['Livestock_Density'] < 5) &\n",
    "    (data['Bamboo_Coverage'] > 20) &\n",
    "    (data['Vegetation_Diversity_Index'] > 0.3) &\n",
    "    (data['Water_Source_Availability'] == 1) &\n",
    "    (data['Human_Disturbance_Index'] < 0.5) &\n",
    "    (data['Slope'] < 20) &\n",
    "    (data['Annual_Rainfall'] > 1000),\n",
    "    1, 0)\n",
    "\n",
    "# Mean Annual Temperature is derived with some noise, considering its correlation with Altitude\n",
    "data['Mean_Annual_Temperature'] = 20 - (data['Altitude'] / 200)  # Simplified linear relation\n",
    "data['Mean_Annual_Temperature'] += np.random.normal(0, 1, n_samples)  # Adding noise\n",
    "\n",
    "# Output the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Save the dataset to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.66623982e-01  1.06349065e-03  7.57121241e-04  2.45211983e-03\n",
      "  -5.34520953e-03  2.06043447e-03 -9.66106843e-02 -1.71885032e-03\n",
      "   7.87332850e-01  7.38661630e-03 -8.45277735e-01]\n",
      " [-8.35174742e-01 -1.02489724e-03 -7.29645787e-04 -2.36313394e-03\n",
      "   5.15123523e-03 -1.98566259e-03  9.31047432e-02  1.65647432e-03\n",
      "  -7.58761035e-01 -7.11856063e-03  8.14603137e-01]]\n",
      "1    50941\n",
      "0    49059\n",
      "Name: Suitable, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming `data` is your DataFrame with all the features but without the 'Suitable' column\n",
    "\n",
    "# Normalize the data for clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Use KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Add the cluster information to the original data\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Check the centroid values to infer which cluster might be 'suitable'\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# Assume cluster 1 is 'suitable' based on centroid analysis\n",
    "data['Suitable'] = (data['Cluster'] == 1).astype(int)\n",
    "\n",
    "# Drop the 'Cluster' column if it's no longer needed\n",
    "data.drop('Cluster', axis=1, inplace=True)\n",
    "\n",
    "# Now, the 'Suitable' column is derived from clustering, which may yield a more nuanced distribution\n",
    "print(data['Suitable'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    50941\n",
       "0    49059\n",
       "Name: Suitable, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Suitable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.7\n",
      "0    0.3\n",
      "Name: Suitable, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `data` is your DataFrame with all the features but without the 'Suitable' column\n",
    "\n",
    "# Normalize the data for clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Use KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Add the cluster information to the original data\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Identify the centroids for each cluster\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Calculate the distance of each point to the centroids\n",
    "distances = kmeans.transform(data_scaled)\n",
    "\n",
    "# Get the distances to the \"farthest\" centroid (assuming cluster 0 is 'unsuitable', cluster 1 is 'suitable')\n",
    "data['Distance_to_Unsuitable'] = distances[:, 0]\n",
    "data['Distance_to_Suitable'] = distances[:, 1]\n",
    "\n",
    "# Determine the 30th percentile threshold for the distance to the 'suitable' centroid\n",
    "threshold = np.percentile(data['Distance_to_Suitable'], 70)  # since we want the closest 30%\n",
    "\n",
    "# Label the data based on the threshold\n",
    "data['Suitable'] = (data['Distance_to_Suitable'] <= threshold).astype(int)\n",
    "\n",
    "# Now, 30% of your data should be labeled as suitable\n",
    "print(data['Suitable'].value_counts(normalize=True))  # Check the proportion of '1's and '0's\n",
    "\n",
    "# Drop the columns not needed anymore\n",
    "data.drop(['Cluster', 'Distance_to_Unsuitable', 'Distance_to_Suitable'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    70000\n",
       "0    30000\n",
       "Name: Suitable, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Suitable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altitude</th>\n",
       "      <th>Distance_from_Human_Paths</th>\n",
       "      <th>Livestock_Density</th>\n",
       "      <th>Vegetation_Diversity_Index</th>\n",
       "      <th>Water_Source_Availability</th>\n",
       "      <th>Human_Disturbance_Index</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Annual_Rainfall</th>\n",
       "      <th>Bamboo_Coverage</th>\n",
       "      <th>Suitable</th>\n",
       "      <th>Mean_Annual_Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.988261</td>\n",
       "      <td>434.678578</td>\n",
       "      <td>0.634095</td>\n",
       "      <td>0.319653</td>\n",
       "      <td>0</td>\n",
       "      <td>1.025552</td>\n",
       "      <td>1.902428</td>\n",
       "      <td>1364.685583</td>\n",
       "      <td>30.270545</td>\n",
       "      <td>1</td>\n",
       "      <td>9.912647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3291.571474</td>\n",
       "      <td>374.299977</td>\n",
       "      <td>0.132676</td>\n",
       "      <td>0.604097</td>\n",
       "      <td>1</td>\n",
       "      <td>1.032245</td>\n",
       "      <td>16.484248</td>\n",
       "      <td>1616.727901</td>\n",
       "      <td>71.382519</td>\n",
       "      <td>1</td>\n",
       "      <td>1.701986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2810.386672</td>\n",
       "      <td>216.189749</td>\n",
       "      <td>0.477984</td>\n",
       "      <td>0.157106</td>\n",
       "      <td>0</td>\n",
       "      <td>0.963946</td>\n",
       "      <td>29.466434</td>\n",
       "      <td>1373.229653</td>\n",
       "      <td>31.969303</td>\n",
       "      <td>1</td>\n",
       "      <td>6.258988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2517.048665</td>\n",
       "      <td>339.831903</td>\n",
       "      <td>0.386706</td>\n",
       "      <td>0.560503</td>\n",
       "      <td>1</td>\n",
       "      <td>1.142016</td>\n",
       "      <td>9.966878</td>\n",
       "      <td>1747.708413</td>\n",
       "      <td>50.076009</td>\n",
       "      <td>1</td>\n",
       "      <td>7.423174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1543.241009</td>\n",
       "      <td>227.141237</td>\n",
       "      <td>0.654869</td>\n",
       "      <td>0.349722</td>\n",
       "      <td>1</td>\n",
       "      <td>1.050573</td>\n",
       "      <td>10.647234</td>\n",
       "      <td>1500.073131</td>\n",
       "      <td>17.449099</td>\n",
       "      <td>0</td>\n",
       "      <td>12.331676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2943.070627</td>\n",
       "      <td>486.628948</td>\n",
       "      <td>1.059066</td>\n",
       "      <td>0.097482</td>\n",
       "      <td>0</td>\n",
       "      <td>1.020356</td>\n",
       "      <td>27.083635</td>\n",
       "      <td>1122.526593</td>\n",
       "      <td>40.315010</td>\n",
       "      <td>1</td>\n",
       "      <td>2.828343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>2914.356369</td>\n",
       "      <td>496.073871</td>\n",
       "      <td>1.447014</td>\n",
       "      <td>0.163341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932492</td>\n",
       "      <td>10.520047</td>\n",
       "      <td>1598.369927</td>\n",
       "      <td>80.260443</td>\n",
       "      <td>1</td>\n",
       "      <td>4.820152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>2683.797496</td>\n",
       "      <td>106.722664</td>\n",
       "      <td>0.262224</td>\n",
       "      <td>0.680997</td>\n",
       "      <td>1</td>\n",
       "      <td>1.095725</td>\n",
       "      <td>11.544332</td>\n",
       "      <td>1523.291354</td>\n",
       "      <td>68.622167</td>\n",
       "      <td>1</td>\n",
       "      <td>6.417745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>2298.783938</td>\n",
       "      <td>529.334414</td>\n",
       "      <td>0.840479</td>\n",
       "      <td>0.315498</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879566</td>\n",
       "      <td>23.171634</td>\n",
       "      <td>1468.113298</td>\n",
       "      <td>22.619058</td>\n",
       "      <td>1</td>\n",
       "      <td>5.281059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>2057.798668</td>\n",
       "      <td>0.780649</td>\n",
       "      <td>0.126169</td>\n",
       "      <td>0.152416</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949579</td>\n",
       "      <td>10.240502</td>\n",
       "      <td>1141.748573</td>\n",
       "      <td>37.591197</td>\n",
       "      <td>1</td>\n",
       "      <td>8.921497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Altitude  Distance_from_Human_Paths  Livestock_Density  \\\n",
       "0      2023.988261                 434.678578           0.634095   \n",
       "1      3291.571474                 374.299977           0.132676   \n",
       "2      2810.386672                 216.189749           0.477984   \n",
       "3      2517.048665                 339.831903           0.386706   \n",
       "4      1543.241009                 227.141237           0.654869   \n",
       "...            ...                        ...                ...   \n",
       "99995  2943.070627                 486.628948           1.059066   \n",
       "99996  2914.356369                 496.073871           1.447014   \n",
       "99997  2683.797496                 106.722664           0.262224   \n",
       "99998  2298.783938                 529.334414           0.840479   \n",
       "99999  2057.798668                   0.780649           0.126169   \n",
       "\n",
       "       Vegetation_Diversity_Index  Water_Source_Availability  \\\n",
       "0                        0.319653                          0   \n",
       "1                        0.604097                          1   \n",
       "2                        0.157106                          0   \n",
       "3                        0.560503                          1   \n",
       "4                        0.349722                          1   \n",
       "...                           ...                        ...   \n",
       "99995                    0.097482                          0   \n",
       "99996                    0.163341                          1   \n",
       "99997                    0.680997                          1   \n",
       "99998                    0.315498                          1   \n",
       "99999                    0.152416                          0   \n",
       "\n",
       "       Human_Disturbance_Index      Slope  Annual_Rainfall  Bamboo_Coverage  \\\n",
       "0                     1.025552   1.902428      1364.685583        30.270545   \n",
       "1                     1.032245  16.484248      1616.727901        71.382519   \n",
       "2                     0.963946  29.466434      1373.229653        31.969303   \n",
       "3                     1.142016   9.966878      1747.708413        50.076009   \n",
       "4                     1.050573  10.647234      1500.073131        17.449099   \n",
       "...                        ...        ...              ...              ...   \n",
       "99995                 1.020356  27.083635      1122.526593        40.315010   \n",
       "99996                 0.932492  10.520047      1598.369927        80.260443   \n",
       "99997                 1.095725  11.544332      1523.291354        68.622167   \n",
       "99998                 0.879566  23.171634      1468.113298        22.619058   \n",
       "99999                 0.949579  10.240502      1141.748573        37.591197   \n",
       "\n",
       "       Suitable  Mean_Annual_Temperature  \n",
       "0             1                 9.912647  \n",
       "1             1                 1.701986  \n",
       "2             1                 6.258988  \n",
       "3             1                 7.423174  \n",
       "4             0                12.331676  \n",
       "...         ...                      ...  \n",
       "99995         1                 2.828343  \n",
       "99996         1                 4.820152  \n",
       "99997         1                 6.417745  \n",
       "99998         1                 5.281059  \n",
       "99999         1                 8.921497  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7459  1482]\n",
      " [ 1478 19581]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      8941\n",
      "           1       0.93      0.93      0.93     21059\n",
      "\n",
      "    accuracy                           0.90     30000\n",
      "   macro avg       0.88      0.88      0.88     30000\n",
      "weighted avg       0.90      0.90      0.90     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "# Features matrix 'X' and target variable 'y'\n",
    "X = data.drop('Suitable', axis=1)\n",
    "y = data['Suitable']\n",
    "\n",
    "# Step 2: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Predictions and evaluation\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
